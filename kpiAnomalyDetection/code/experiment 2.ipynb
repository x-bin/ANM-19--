{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('train_feature.txt','rb')\n",
    "\n",
    "timeseries_features_new = pickle.load(file)\n",
    "timeseries_features_label_new = pickle.load(file)\n",
    "timeseries_features_label_vital_new = pickle.load(file)\n",
    "scaler_list_new = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2450341, 128) (2450341,) (2450341,)\n"
     ]
    }
   ],
   "source": [
    "print(timeseries_features_new.shape,timeseries_features_label_new.shape,timeseries_features_label_vital_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53219 4853\n"
     ]
    }
   ],
   "source": [
    "print(sum(timeseries_features_label_new),sum(timeseries_features_label_vital_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 [StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True)]\n"
     ]
    }
   ],
   "source": [
    "print(len(scaler_list_new),scaler_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('test_feature.txt','rb')\n",
    "testseries_features_new = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2319237, 128)\n"
     ]
    }
   ],
   "source": [
    "print(testseries_features_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np   \n",
    "import sklearn\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../input/train.csv')\n",
    "test_data = pd.read_csv('../../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('train_test_all.txt','rb')\n",
    "timeseries_all = pickle.load(file)\n",
    "timeseries_label = pickle.load(file)\n",
    "testseries_all = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128562 112627\n",
      "128613 110185\n",
      "129035 112113\n",
      "128679 108747\n",
      "129128 111171\n",
      "128971 111004\n",
      "128667 112786\n",
      "128853 112116\n",
      "8784 8784\n",
      "10960 10960\n",
      "8248 8249\n",
      "8247 8248\n",
      "8784 8784\n",
      "8784 8784\n",
      "8784 8784\n",
      "147024 147024\n",
      "147009 147010\n",
      "100254 108901\n",
      "147629 147629\n",
      "147668 147669\n",
      "147689 147690\n",
      "137925 137925\n",
      "129453 129454\n",
      "147680 147681\n",
      "65449 65450\n",
      "65436 65436\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(timeseries_all)):\n",
    "    print(len(timeseries_all[i]),len(testseries_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25974\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "data_features_diff = len(test_data) - len(testseries_features_new)\n",
    "print(data_features_diff)\n",
    "data_features_diff_avg = int(data_features_diff / len(testseries_all))\n",
    "print(data_features_diff_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StratifiedKFold in module sklearn.model_selection._split:\n",
      "\n",
      "class StratifiedKFold(_BaseKFold)\n",
      " |  StratifiedKFold(n_splits='warn', shuffle=False, random_state=None)\n",
      " |  \n",
      " |  Stratified K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets.\n",
      " |  \n",
      " |  This cross-validation object is a variation of KFold that returns\n",
      " |  stratified folds. The folds are made by preserving the percentage of\n",
      " |  samples for each class.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          ``n_splits`` default value will change from 3 to 5 in v0.22.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle each class's samples before splitting into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``shuffle`` == True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import StratifiedKFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([0, 0, 1, 1])\n",
      " |  >>> skf = StratifiedKFold(n_splits=2)\n",
      " |  >>> skf.get_n_splits(X, y)\n",
      " |  2\n",
      " |  >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in skf.split(X, y):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [1 3] TEST: [0 2]\n",
      " |  TRAIN: [0 2] TEST: [1 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Train and test sizes may be different in each fold, with a difference of at\n",
      " |  most ``n_classes``.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StratifiedKFold\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits='warn', shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  split(self, X, y, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |          Note that providing ``y`` is sufficient to generate the splits and\n",
      " |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      " |          ``X`` instead of actual training data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |          Stratification is done based on the y labels.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting ``random_state``\n",
      " |      to an integer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,)\n",
      "[1. 1. 1. ... 0. 0. 0.] 4799357.0\n",
      "[1. 1. 1. ... 5. 5. 5.] 5065452.0\n"
     ]
    }
   ],
   "source": [
    "ratio = round((len(timeseries_features_label_new) - sum(timeseries_features_label_new)) * 0.1 / sum(timeseries_features_label_new))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label_new)) - timeseries_features_label_new\n",
    "print(non_anomaly,non_anomaly.shape)\n",
    "sample_ratio = (99*ratio) * timeseries_features_label_vital_new + non_anomaly\n",
    "print(sample_ratio,sum(sample_ratio))\n",
    "sample_ratio = sample_ratio + ratio * timeseries_features_label_new\n",
    "print(sample_ratio,sum(sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2205306/2205306 [==============================] - 25s 11us/step - loss: 0.9727 - binary_accuracy: 0.7564\n",
      "Epoch 2/10\n",
      "2205306/2205306 [==============================] - 19s 9us/step - loss: 0.7813 - binary_accuracy: 0.8538\n",
      "Epoch 3/10\n",
      "2205306/2205306 [==============================] - 19s 9us/step - loss: 0.6828 - binary_accuracy: 0.8896\n",
      "Epoch 4/10\n",
      "2205306/2205306 [==============================] - 19s 9us/step - loss: 0.6176 - binary_accuracy: 0.9060\n",
      "Epoch 5/10\n",
      "2205306/2205306 [==============================] - 19s 9us/step - loss: 0.5741 - binary_accuracy: 0.9157\n",
      "Epoch 6/10\n",
      "2205306/2205306 [==============================] - 19s 9us/step - loss: 0.5438 - binary_accuracy: 0.9243\n",
      "Epoch 7/10\n",
      "2205306/2205306 [==============================] - 19s 9us/step - loss: 0.5264 - binary_accuracy: 0.9285\n",
      "Epoch 8/10\n",
      "2205306/2205306 [==============================] - 19s 9us/step - loss: 0.5057 - binary_accuracy: 0.9329\n",
      "Epoch 9/10\n",
      "2205306/2205306 [==============================] - 19s 9us/step - loss: 0.4942 - binary_accuracy: 0.9366\n",
      "Epoch 10/10\n",
      "2205306/2205306 [==============================] - 19s 9us/step - loss: 0.4813 - binary_accuracy: 0.9382\n",
      "Round 1 :\n",
      "\n",
      "\n",
      "2205306/2205306 [==============================] - 7s 3us/step\n",
      "245035/245035 [==============================] - 0s 2us/step\n",
      "threshhold: 0.5 training data\n",
      "0.36899433370021606\n",
      "0.9449234816376808\n",
      "0.5307354941601389\n",
      "test data\n",
      "0.36811977102597976\n",
      "0.9425028184892897\n",
      "0.5294490183660545\n",
      "threshhold: 0.6 training data\n",
      "0.4630185219909578\n",
      "0.9279704365617888\n",
      "0.6177870749386688\n",
      "test data\n",
      "0.4623181604880338\n",
      "0.9255918827508456\n",
      "0.6166364148463417\n",
      "threshhold: 0.7 training data\n",
      "0.5791337473675738\n",
      "0.9014134496941353\n",
      "0.7051973082451326\n",
      "test data\n",
      "0.5752681692177896\n",
      "0.8968432919954904\n",
      "0.7009325207430795\n",
      "threshhold: 0.8 training data\n",
      "0.7172148951328433\n",
      "0.8617449944672944\n",
      "0.7828651632115016\n",
      "test data\n",
      "0.7132239083307526\n",
      "0.8654641112363773\n",
      "0.7820033955857386\n",
      "threshhold: 0.9 training data\n",
      "0.8582896873499074\n",
      "0.7834937469987682\n",
      "0.8191879502292077\n",
      "test data\n",
      "0.8589194699286442\n",
      "0.791619691845171\n",
      "0.8238975261562531\n",
      "threshhold: 0.95 training data\n",
      "0.9198384127071367\n",
      "0.6988329122909577\n",
      "0.7942481550909997\n",
      "test data\n",
      "0.9195289499509323\n",
      "0.7042465238632093\n",
      "0.7976165141519472\n",
      "threshhold: 0.99 training data\n",
      "0.9680241560854754\n",
      "0.5220786270538864\n",
      "0.6783219628640019\n",
      "test data\n",
      "0.9680741503604532\n",
      "0.5298759864712514\n",
      "0.6848816029143899\n",
      "Epoch 1/10\n",
      " 315000/2205306 [===>..........................] - ETA: 53s - loss: 1.3821 - binary_accuracy: 0.6079"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1f1fa49c5027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     h = m.fit(timeseries_features_new[train_index], timeseries_features_label_new[train_index], epochs=10, batch_size=5000, verbose=1,\n\u001b[0;32m---> 23\u001b[0;31m                    sample_weight=sample_ratio[train_index])\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Round\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\":\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeseries_features_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for train_index, test_index in kfold.split(timeseries_features_new, timeseries_features_label_new):\n",
    "    count += 1\n",
    "    m = Sequential()\n",
    "    m.add(Dense(128, input_dim = 128, kernel_regularizer=l2(0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(Dropout(0.5))\n",
    "\n",
    "    m.add(Dense(64, kernel_regularizer=l2(0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('relu'))\n",
    "    m.add(Dropout(0.5))\n",
    "\n",
    "    m.add(Dense(1, kernel_regularizer=l2(0.0001)))\n",
    "    m.add(BatchNormalization())\n",
    "    m.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "    m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "    \n",
    "    h = m.fit(timeseries_features_new[train_index], timeseries_features_label_new[train_index], epochs=10, batch_size=5000, verbose=1,\n",
    "                   sample_weight=sample_ratio[train_index])\n",
    "    print(\"Round\",count,\":\\n\\n\")\n",
    "    p_train = m.predict(timeseries_features_new[train_index], batch_size=5000,verbose=1)\n",
    "    p_test = m.predict(timeseries_features_new[test_index], batch_size=5000,verbose=1)\n",
    "    t_list = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "    for threshhold in t_list:\n",
    "        train_data_check = np.ravel(p_train > threshhold).astype(int)\n",
    "        print(\"threshhold:\",threshhold,\"training data\")\n",
    "        print(precision_score(timeseries_features_label_new[train_index], train_data_check))\n",
    "        print(recall_score(timeseries_features_label_new[train_index], train_data_check))\n",
    "        print(f1_score(timeseries_features_label_new[train_index], train_data_check))\n",
    "        test_data_check = np.ravel(p_test > threshhold).astype(int)\n",
    "        print(\"test data\")\n",
    "        print(precision_score(timeseries_features_label_new[test_index], test_data_check))\n",
    "        print(recall_score(timeseries_features_label_new[test_index], test_data_check))\n",
    "        print(f1_score(timeseries_features_label_new[test_index], test_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2450341/2450341 [==============================] - 31s 13us/step - loss: 0.9051 - binary_accuracy: 0.7722\n",
      "Epoch 2/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.7713 - binary_accuracy: 0.8821\n",
      "Epoch 3/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.7127 - binary_accuracy: 0.9030\n",
      "Epoch 4/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.6773 - binary_accuracy: 0.9111\n",
      "Epoch 5/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.6538 - binary_accuracy: 0.9145\n",
      "Epoch 6/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.6326 - binary_accuracy: 0.9162\n",
      "Epoch 7/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.6103 - binary_accuracy: 0.9171\n",
      "Epoch 8/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5952 - binary_accuracy: 0.9169\n",
      "Epoch 9/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5909 - binary_accuracy: 0.9218\n",
      "Epoch 10/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5735 - binary_accuracy: 0.9209\n",
      "Epoch 11/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.5543 - binary_accuracy: 0.9219\n",
      "Epoch 12/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5504 - binary_accuracy: 0.9207\n",
      "Epoch 13/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5425 - binary_accuracy: 0.9263\n",
      "Epoch 14/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.5337 - binary_accuracy: 0.9272\n",
      "Epoch 15/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5198 - binary_accuracy: 0.9273\n",
      "Epoch 16/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5215 - binary_accuracy: 0.9278\n",
      "Epoch 17/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.5102 - binary_accuracy: 0.9289\n",
      "Epoch 18/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5116 - binary_accuracy: 0.9288\n",
      "Epoch 19/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5052 - binary_accuracy: 0.9312\n",
      "Epoch 20/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.5012 - binary_accuracy: 0.9318\n",
      "Epoch 21/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.4932 - binary_accuracy: 0.9318\n",
      "Epoch 22/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.4935 - binary_accuracy: 0.9313\n",
      "Epoch 23/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.4908 - binary_accuracy: 0.9316\n",
      "Epoch 24/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.4815 - binary_accuracy: 0.9325\n",
      "Epoch 25/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.4763 - binary_accuracy: 0.9342\n",
      "Epoch 26/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.4740 - binary_accuracy: 0.9363\n",
      "Epoch 27/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.4694 - binary_accuracy: 0.9352\n",
      "Epoch 28/30\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.4669 - binary_accuracy: 0.9354\n",
      "Epoch 29/30\n",
      "2450341/2450341 [==============================] - 21s 8us/step - loss: 0.4643 - binary_accuracy: 0.9347\n",
      "Epoch 30/30\n",
      "2450341/2450341 [==============================] - 22s 9us/step - loss: 0.4645 - binary_accuracy: 0.9351\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(128, input_dim = 128))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dropout(0.5))\n",
    "\n",
    "m.add(Dense(64))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dropout(0.5))\n",
    "\n",
    "m.add(Dense(1))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "h = m.fit(timeseries_features_new, timeseries_features_label_new, epochs=30, batch_size=5000, verbose=1,\n",
    "               sample_weight=sample_ratio_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450341/2450341 [==============================] - 9s 4us/step\n"
     ]
    }
   ],
   "source": [
    "p = m.predict(timeseries_features_new, batch_size=5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0031930249708101853\n",
      "0.7627811860940695\n",
      "0.11214040098461076\n",
      "0.19553429549661713\n"
     ]
    }
   ],
   "source": [
    "train_data_check = np.ravel(p>0.96).astype(int)\n",
    "print(sum(train_data_check)/len(train_data_check))\n",
    "print(precision_score(timeseries_features_label_new, train_data_check))\n",
    "print(recall_score(timeseries_features_label_new, train_data_check))\n",
    "print(f1_score(timeseries_features_label_new, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128562 127563\n",
      "257175 255177\n",
      "386210 383213\n",
      "514889 510893\n",
      "644017 639022\n",
      "772988 766994\n",
      "901655 894662\n",
      "1030508 1022516\n",
      "1039292 1030301\n",
      "1050252 1040262\n",
      "1058500 1047511\n",
      "1066747 1054759\n",
      "1075531 1062544\n",
      "1084315 1070329\n",
      "1093099 1078114\n",
      "1240123 1224139\n",
      "1387132 1370149\n",
      "1487386 1469404\n",
      "1635015 1616034\n",
      "1782683 1762703\n",
      "1930372 1909393\n",
      "2068297 2046319\n",
      "2197750 2174773\n",
      "2345430 2321454\n",
      "2410879 2385904\n",
      "2476315 2450341\n",
      "2476315\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "evaluation_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(timeseries_all)):\n",
    "    next_index += len(timeseries_all[i]) - data_features_diff_avg\n",
    "    evaluation_new = np.concatenate((evaluation_new, train_data_check[last_index : next_index]))\n",
    "    print(len(evaluation_new),next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(timeseries_all)-1:\n",
    "        evaluation_new = np.concatenate((evaluation_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(evaluation_new))\n",
    "assert(len(evaluation_new) == len(train_data))\n",
    "evaluation_new = evaluation_new.astype(int)\n",
    "evaluation_df = pd.DataFrame({'KPI ID': train_data['KPI ID'], \n",
    "                         'timestamp': train_data['timestamp'], \n",
    "                         'predict': evaluation_new})\n",
    "evaluation_df.to_csv('evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0\n",
      "OMP: Info #156: KMP_AFFINITY: 1 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #159: KMP_AFFINITY: 1 packages x 1 cores/pkg x 1 threads/core (1 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 47124 tid 47124 thread 0 bound to OS proc set 0\n",
      "{\"result\": true, \"data\": 0.6806020664751941, \"message\": \"计算成功\"}\n"
     ]
    }
   ],
   "source": [
    "!python evaluation.py \"../../input/train.csv\" \"evaluation.csv\" 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319237/2319237 [==============================] - 7s 3us/step\n"
     ]
    }
   ],
   "source": [
    "pm_t = m.predict(testseries_features_new,batch_size=5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.004897300275909707\n"
     ]
    }
   ],
   "source": [
    "predict_flagm = (np.ravel(pm_t)>0.96).astype(int)\n",
    "print(predict_flagm)\n",
    "print(sum(predict_flagm)/len(predict_flagm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flagm[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predicta5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_delay_label(label, delay):\n",
    "    splits = np.where(label[1:] != label[:-1])[0] + 1\n",
    "    vital_label = np.zeros(len(label)).astype(int)\n",
    "    is_anomaly = label[0] == 1\n",
    "    pos = 0\n",
    "    for sp in splits:\n",
    "        if is_anomaly:\n",
    "            vital_label[pos:min(pos + delay + 1, sp)] = 1\n",
    "        #print(is_anomaly, pos, min(pos + delay + 1, sp),\n",
    "         #     vital_label[pos:min(pos + delay + 1, sp)],label[pos:min(pos + delay + 1, sp)])\n",
    "        is_anomaly = not is_anomaly\n",
    "        pos = sp\n",
    "    sp = len(label)\n",
    "    if is_anomaly:\n",
    "        vital_label[pos:min(pos + delay + 1, sp)] = 1\n",
    "    return vital_label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5189 2450341\n"
     ]
    }
   ],
   "source": [
    "vital_label = get_first_delay_label(timeseries_features_label_new,7)\n",
    "print(sum(vital_label),len(vital_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4853\n"
     ]
    }
   ],
   "source": [
    "print(sum(timeseries_features_label_vital_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2397122 53219 5189 453.0\n"
     ]
    }
   ],
   "source": [
    "sum_non = len(timeseries_features_label_new) - sum(timeseries_features_label_new)\n",
    "vital_ratio = round((sum_non - sum(timeseries_features_label_new) + sum(vital_label))/sum(vital_label))\n",
    "print(sum_non,sum(timeseries_features_label_new),sum(vital_label),vital_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800958.0\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "sample_ratio_new = vital_ratio * vital_label + 1\n",
    "print(sum(sample_ratio_new))\n",
    "print(sample_ratio_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 :\n",
      "\n",
      "\n",
      "threshhold: 0.7\n",
      "training data\n",
      "0.5645237697152092\n",
      "0.8459193686452179\n",
      "0.6771511418997401\n",
      "test data\n",
      "0.5681619392727727\n",
      "0.8543780533633972\n",
      "0.6824765478424015\n",
      "threshhold: 0.8\n",
      "training data\n",
      "0.716100659662839\n",
      "0.7955195523727999\n",
      "0.7537238146104088\n",
      "test data\n",
      "0.720536462699078\n",
      "0.8075911311537016\n",
      "0.7615841233277222\n",
      "threshhold: 0.9\n",
      "training data\n",
      "0.8663378411339061\n",
      "0.6674113201244337\n",
      "0.7539742440681165\n",
      "test data\n",
      "0.867275787448906\n",
      "0.6777527245396467\n",
      "0.7608902014555426\n",
      "threshhold: 0.95\n",
      "training data\n",
      "0.9271312977781752\n",
      "0.4329916278681337\n",
      "0.5902997182136452\n",
      "test data\n",
      "0.9203264671589584\n",
      "0.444945509207065\n",
      "0.5998733375554148\n",
      "threshhold: 0.99\n",
      "training data\n",
      "0.9914738124238733\n",
      "0.11896360941186296\n",
      "0.21243755126388789\n",
      "test data\n",
      "0.9940209267563528\n",
      "0.12495302517850432\n",
      "0.22199966616591552\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for train_index, test_index in kfold.split(timeseries_features_new, timeseries_features_label_new):\n",
    "    count += 1\n",
    "    xgb_model = xgb.XGBClassifier(n_jobs=10, verbosity=1)\n",
    "    \n",
    "    xgb_model.fit(timeseries_features_new, timeseries_features_label_new, \n",
    "                  sample_weight = xgb_sample_ratio, verbose = True)\n",
    "    print(\"Round\",count,\":\\n\\n\")\n",
    "    p_train = (xgb_model.predict_proba(timeseries_features_new[train_index]))[:,1:]\n",
    "    p_test = (xgb_model.predict_proba(timeseries_features_new[test_index]))[:,1:]\n",
    "    t_list = [0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "    for threshhold in t_list:\n",
    "        train_data_check = np.ravel(p_train > threshhold).astype(int)\n",
    "        print(\"threshhold:\",threshhold)\n",
    "        print(\"training data\")\n",
    "        print(precision_score(timeseries_features_label_new[train_index], train_data_check))\n",
    "        print(recall_score(timeseries_features_label_new[train_index], train_data_check))\n",
    "        print(f1_score(timeseries_features_label_new[train_index], train_data_check))\n",
    "        \n",
    "        test_data_check = np.ravel(p_test > threshhold).astype(int)\n",
    "        print(\"test data\")\n",
    "        print(precision_score(timeseries_features_label_new[test_index], test_data_check))\n",
    "        print(recall_score(timeseries_features_label_new[test_index], test_data_check))\n",
    "        print(f1_score(timeseries_features_label_new[test_index], test_data_check))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_jobs=10, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,)\n",
      "[1. 1. 1. ... 0. 0. 0.] 3471245.0\n",
      "[ 1.  1.  1. ... 23. 23. 23.] 4695282.0\n"
     ]
    }
   ],
   "source": [
    "ratio = round((len(timeseries_features_label_new) - sum(timeseries_features_label_new)) * 0.5 / sum(timeseries_features_label_new))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label_new)) - timeseries_features_label_new\n",
    "print(non_anomaly,non_anomaly.shape)\n",
    "xgb_sample_ratio = (9*ratio) * vital_label + non_anomaly\n",
    "print(xgb_sample_ratio,sum(xgb_sample_ratio))\n",
    "xgb_sample_ratio = xgb_sample_ratio + ratio * timeseries_features_label_new\n",
    "print(xgb_sample_ratio,sum(xgb_sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:40:19] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:23] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:25] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:28] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:31] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:34] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:37] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:40] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:43] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:46] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:48] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:51] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:40:57] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:00] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:03] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:06] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:09] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:11] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:14] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:17] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:20] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:23] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:26] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:28] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:31] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:34] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:37] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:40] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:42] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:48] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:51] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:56] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:41:59] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:02] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:05] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:07] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:10] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:13] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:16] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:18] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:21] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:24] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:27] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:30] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:32] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:35] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:38] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:41] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:44] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:46] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:49] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:52] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:55] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:42:57] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:00] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:03] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:06] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:09] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:12] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:14] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:17] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:43:20] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:23] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:25] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:28] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:31] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:34] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:36] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:39] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:42] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:44] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:47] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:50] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:53] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:56] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:43:58] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:01] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:04] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:07] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:09] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:12] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:15] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:18] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:20] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:23] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:26] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:29] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:31] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:34] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:37] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:40] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:43] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:48] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:51] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[10:44:56] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=10,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=2)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(timeseries_features_new, timeseries_features_label_new, sample_weight = xgb_sample_ratio, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = xgb_model.predict_proba(timeseries_features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2450341,)\n",
      "0.006153021150933686\n",
      "0.9822245804868343\n",
      "0.27826528119656513\n",
      "0.43367107883331374\n"
     ]
    }
   ],
   "source": [
    "train_data_check = (np.ravel(y_p[:,1:])>0.98).astype(int)\n",
    "print(train_data_check.shape)\n",
    "print(sum(train_data_check)/len(train_data_check))\n",
    "print(precision_score(timeseries_features_label_new, train_data_check))\n",
    "print(recall_score(timeseries_features_label_new, train_data_check))\n",
    "print(f1_score(timeseries_features_label_new, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128562 127563\n",
      "257175 255177\n",
      "386210 383213\n",
      "514889 510893\n",
      "644017 639022\n",
      "772988 766994\n",
      "901655 894662\n",
      "1030508 1022516\n",
      "1039292 1030301\n",
      "1050252 1040262\n",
      "1058500 1047511\n",
      "1066747 1054759\n",
      "1075531 1062544\n",
      "1084315 1070329\n",
      "1093099 1078114\n",
      "1240123 1224139\n",
      "1387132 1370149\n",
      "1487386 1469404\n",
      "1635015 1616034\n",
      "1782683 1762703\n",
      "1930372 1909393\n",
      "2068297 2046319\n",
      "2197750 2174773\n",
      "2345430 2321454\n",
      "2410879 2385904\n",
      "2476315 2450341\n",
      "2476315\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "evaluation_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(timeseries_all)):\n",
    "    next_index += len(timeseries_all[i]) - data_features_diff_avg\n",
    "    evaluation_new = np.concatenate((evaluation_new, train_data_check[last_index : next_index]))\n",
    "    print(len(evaluation_new),next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(timeseries_all)-1:\n",
    "        evaluation_new = np.concatenate((evaluation_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(evaluation_new))\n",
    "assert(len(evaluation_new) == len(train_data))\n",
    "evaluation_new = evaluation_new.astype(int)\n",
    "evaluation_df = pd.DataFrame({'KPI ID': train_data['KPI ID'], \n",
    "                         'timestamp': train_data['timestamp'], \n",
    "                         'predict': evaluation_new})\n",
    "evaluation_df.to_csv('evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0\n",
      "OMP: Info #156: KMP_AFFINITY: 1 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #159: KMP_AFFINITY: 1 packages x 1 cores/pkg x 1 threads/core (1 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 17116 tid 17116 thread 0 bound to OS proc set 0\n",
      "{\"result\": true, \"data\": 0.8314651303567328, \"message\": \"计算成功\"}\n"
     ]
    }
   ],
   "source": [
    "!python evaluation.py \"../../input/train.csv\" \"evaluation.csv\" 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t = xgb_model.predict_proba(testseries_features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.0023279207773936\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(p_t[:,1:])>0.98).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004897300275909707 0.004651529791910012\n",
      "[0 0 0 ... 0 0 0]\n",
      "0.0072454863388260885\n"
     ]
    }
   ],
   "source": [
    "predict_t1 = (np.ravel(p_t[:,1:])>0.98)\n",
    "predict_t2 = np.ravel(pm_t)>0.96\n",
    "predict_t3 = (np.ravel(p_test_test[:,1:]) > 0.9325)\n",
    "predict_xg = ((predict_t1 | predict_t3)).astype(int)\n",
    "predict_flag = (predict_t2 | predict_xg).astype(int)\n",
    "print(sum(predict_t2)/len(predict_t2), sum(predict_xg)/len(predict_xg))\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2450341,)\n",
      "(2450341,)\n"
     ]
    }
   ],
   "source": [
    "train_temp1 = np.ravel(p)\n",
    "print(train_temp1.shape)\n",
    "train_temp2 = np.ravel(y_p[:,1:])\n",
    "print(train_temp2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.007216954701406865\n",
      "0.8724836009952499\n",
      "0.2899152558296849\n",
      "0.43521430686994916\n"
     ]
    }
   ],
   "source": [
    "train_data_check = ((train_temp1*0.7 + train_temp2 * 0.3)>0.9).astype(int)\n",
    "print(train_data_check)\n",
    "print(sum(train_data_check)/len(train_data_check))\n",
    "print(precision_score(timeseries_features_label_new, train_data_check))\n",
    "print(recall_score(timeseries_features_label_new, train_data_check))\n",
    "print(f1_score(timeseries_features_label_new, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.011411942807052492\n"
     ]
    }
   ],
   "source": [
    "predict_flagm = (np.ravel(pm_t)>0.85).astype(int)\n",
    "print(predict_flagm)\n",
    "print(sum(predict_flagm)/len(predict_flagm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.221663   0.13683766 0.11092606 ... 0.16910532 0.16221571 0.15728143]\n"
     ]
    }
   ],
   "source": [
    "temp1 = np.ravel(pm_t)\n",
    "print(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.011036819436737169\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(p_t[:,1:])>0.9).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11381663 0.21982214 0.19585699 ... 0.6884514  0.71765184 0.6952185 ]\n"
     ]
    }
   ],
   "source": [
    "temp2 = np.ravel(p_t[:,1:])\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.00589633573455408\n"
     ]
    }
   ],
   "source": [
    "predict_flag = ((temp1*0.7 + temp2 * 0.3)>0.85).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictb3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('xgboost.txt','rb')\n",
    "xgb_model_original = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,)\n",
      "[1. 1. 1. ... 0. 0. 0.] 4981244.0\n",
      "[1. 1. 1. ... 2. 2. 2.] 5087682.0\n"
     ]
    }
   ],
   "source": [
    "ratio = round((len(timeseries_features_label_new) - sum(timeseries_features_label_new)) * 0.05 / sum(timeseries_features_label_new))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label_new)) - timeseries_features_label_new\n",
    "print(non_anomaly,non_anomaly.shape)\n",
    "xgb_sample_ratio = (239*ratio) * vital_label + non_anomaly\n",
    "print(xgb_sample_ratio,sum(xgb_sample_ratio))\n",
    "xgb_sample_ratio = xgb_sample_ratio + ratio * timeseries_features_label_new\n",
    "print(xgb_sample_ratio,sum(xgb_sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.9824758e-03 1.3047622e-03 0.0000000e+00 2.4752015e-02 0.0000000e+00\n",
      " 1.6563017e-02 3.4112965e-03 1.2314280e-02 1.1824915e-02 1.1404434e-02\n",
      " 3.4108406e-03 2.8897815e-03 0.0000000e+00 5.8032260e-03 0.0000000e+00\n",
      " 1.1023236e-03 0.0000000e+00 5.0067063e-03 8.6255493e-03 4.7814506e-03\n",
      " 1.9957749e-02 4.4241104e-02 2.2151593e-05 2.5610323e-03 2.1993625e-03\n",
      " 1.7483914e-03 3.9343075e-03 4.6621081e-03 1.1593622e-02 1.4758931e-02\n",
      " 5.0650006e-03 4.4379572e-06 1.3486209e-02 1.1500513e-01 0.0000000e+00\n",
      " 5.8656516e-03 0.0000000e+00 8.6713806e-03 2.1227219e-03 1.4631709e-02\n",
      " 8.9146066e-03 4.0580314e-02 8.6860061e-03 1.7616397e-02 1.0436235e-02\n",
      " 3.2610077e-02 0.0000000e+00 6.3901916e-03 4.9417901e-07 2.1208527e-02\n",
      " 0.0000000e+00 5.1374873e-03 1.6504455e-02 4.0854238e-02 0.0000000e+00\n",
      " 4.4543757e-03 3.7033465e-03 2.6178181e-02 0.0000000e+00 3.2410277e-03\n",
      " 2.8820281e-04 0.0000000e+00 2.1728808e-03 1.2191213e-03 7.3412429e-03\n",
      " 2.8603785e-03 1.8407607e-03 4.6718251e-03 8.8167088e-03 8.7471111e-03\n",
      " 4.5133222e-02 0.0000000e+00 2.7664891e-03 5.7535926e-03 1.6273060e-03\n",
      " 8.5233431e-03 5.0443662e-03 5.9000035e-03 0.0000000e+00 0.0000000e+00\n",
      " 4.1107889e-03 8.4954156e-03 6.7483154e-03 3.5288983e-03 3.2220113e-03\n",
      " 0.0000000e+00 5.2802527e-04 1.7073441e-03 2.9756520e-03 7.9319077e-03\n",
      " 1.4666984e-02 1.8531054e-04 1.1981871e-02 3.9365902e-03 0.0000000e+00\n",
      " 0.0000000e+00 2.3196177e-03 0.0000000e+00 0.0000000e+00 4.5426986e-03\n",
      " 1.4653486e-03 7.6546837e-03 0.0000000e+00 0.0000000e+00 2.0760901e-03\n",
      " 8.1010954e-03 3.2942751e-03 1.7146424e-03 0.0000000e+00 4.3914578e-04\n",
      " 3.6860269e-03 7.5926408e-03 3.9425585e-03 1.8494658e-02 0.0000000e+00\n",
      " 6.1481404e-03 8.4790200e-02 2.4627999e-03 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 4.8596188e-03 3.1216920e-03 3.9934297e-03 3.3749600e-03\n",
      " 4.6361485e-03 0.0000000e+00 2.3398602e-03]\n"
     ]
    }
   ],
   "source": [
    "importance = xgb_model.feature_importances_\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 0.11500513\n",
      "116 0.0847902\n",
      "70 0.045133222\n",
      "21 0.044241104\n",
      "53 0.040854238\n",
      "41 0.040580314\n",
      "45 0.032610077\n",
      "57 0.026178181\n",
      "3 0.024752015\n",
      "49 0.021208527\n",
      "20 0.01995775\n",
      "113 0.018494658\n",
      "43 0.017616397\n",
      "5 0.016563017\n",
      "52 0.016504455\n",
      "29 0.014758931\n",
      "90 0.014666984\n",
      "39 0.014631709\n",
      "32 0.013486209\n",
      "7 0.01231428\n",
      "92 0.011981871\n",
      "8 0.011824915\n",
      "28 0.011593622\n",
      "9 0.011404434\n",
      "44 0.010436235\n",
      "40 0.008914607\n",
      "68 0.008816709\n",
      "69 0.008747111\n",
      "42 0.008686006\n",
      "37 0.008671381\n",
      "18 0.008625549\n",
      "75 0.008523343\n",
      "81 0.008495416\n",
      "105 0.008101095\n",
      "89 0.007931908\n",
      "101 0.0076546837\n",
      "111 0.007592641\n",
      "64 0.007341243\n",
      "82 0.0067483154\n",
      "47 0.0063901916\n",
      "115 0.0061481404\n",
      "0 0.005982476\n",
      "77 0.0059000035\n",
      "35 0.0058656516\n",
      "13 0.005803226\n",
      "73 0.0057535926\n",
      "51 0.0051374873\n",
      "30 0.0050650006\n",
      "76 0.0050443662\n",
      "17 0.0050067063\n",
      "121 0.004859619\n",
      "19 0.0047814506\n",
      "67 0.004671825\n",
      "27 0.004662108\n",
      "125 0.0046361485\n",
      "99 0.0045426986\n",
      "55 0.0044543757\n",
      "80 0.004110789\n",
      "123 0.0039934297\n",
      "112 0.0039425585\n",
      "93 0.00393659\n",
      "26 0.0039343075\n",
      "56 0.0037033465\n",
      "110 0.003686027\n",
      "83 0.0035288983\n",
      "6 0.0034112965\n",
      "10 0.0034108406\n",
      "124 0.00337496\n",
      "106 0.0032942751\n",
      "59 0.0032410277\n",
      "84 0.0032220113\n",
      "122 0.003121692\n",
      "88 0.002975652\n",
      "11 0.0028897815\n",
      "65 0.0028603785\n",
      "72 0.002766489\n",
      "23 0.0025610323\n",
      "117 0.0024628\n",
      "127 0.0023398602\n",
      "96 0.0023196177\n",
      "24 0.0021993625\n",
      "62 0.0021728808\n",
      "38 0.0021227219\n",
      "104 0.00207609\n",
      "66 0.0018407607\n",
      "25 0.0017483914\n",
      "107 0.0017146424\n",
      "87 0.0017073441\n",
      "74 0.001627306\n",
      "100 0.0014653486\n",
      "1 0.0013047622\n",
      "63 0.0012191213\n",
      "15 0.0011023236\n",
      "[0, 1, 3, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 87, 88, 89, 90, 92, 93, 96, 99, 100, 101, 104, 105, 106, 107, 110, 111, 112, 113, 115, 116, 117, 121, 122, 123, 124, 125, 127] 93\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "sort_index = np.argsort(importance)\n",
    "importance_index = []\n",
    "for i in range(len(sort_index)):\n",
    "    if importance[sort_index[len(importance)-1-i]] > 0.001:\n",
    "        importance_index.append(sort_index[len(importance)-1-i])\n",
    "        sum1 += importance[sort_index[len(importance)-1-i]]\n",
    "        print(sort_index[len(importance)-1-i],importance[sort_index[len(importance)-1-i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_test = xgb.XGBClassifier(n_jobs=10, verbosity=2, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:56:36] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:56:41] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:56:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:56:50] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:56:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:56:58] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:03] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:07] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:11] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:16] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:20] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:24] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:29] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:33] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:37] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:42] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:46] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:50] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:55] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:57:59] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:03] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:08] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:12] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:16] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:20] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:25] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:29] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:33] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:38] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:42] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:46] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:51] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:55] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:58:59] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:03] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:08] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:12] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:16] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:20] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:25] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:29] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:33] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:37] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:42] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:46] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:50] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:55] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:59:59] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:03] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:08] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:12] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:16] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:21] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:25] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:29] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:34] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:38] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:43] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:47] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:51] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:00:56] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:00] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:04] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:08] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:01:12] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:17] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:21] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:25] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:29] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:34] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:38] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:43] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:47] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:51] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:01:55] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:00] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:04] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:08] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:13] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:17] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:22] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:26] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:30] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:34] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:39] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:43] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:47] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:51] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:02:56] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:00] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:05] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:09] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:13] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:18] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:22] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:26] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:30] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:35] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:39] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[11:03:44] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=10,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=2)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_test.fit(timeseries_features_new[:,importance_index], timeseries_features_label_new, sample_weight = sample_ratio_new, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = xgb_model_test.predict_proba(timeseries_features_new[:,importance_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2450341,) [0 0 0 ... 0 0 0]\n",
      "0.00728429226789251\n",
      "0.8610006162810242\n",
      "0.28876904864803926\n",
      "0.4324871953621884\n"
     ]
    }
   ],
   "source": [
    "train_data_check = (np.ravel(y_test[:,1:])>0.93).astype(int)\n",
    "print(train_data_check.shape, train_data_check)\n",
    "print(sum(train_data_check)/len(train_data_check))\n",
    "print(precision_score(timeseries_features_label_new, train_data_check))\n",
    "print(recall_score(timeseries_features_label_new, train_data_check))\n",
    "print(f1_score(timeseries_features_label_new, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  max_depth : int\n",
      " |      Maximum tree depth for base learners.\n",
      " |  learning_rate : float\n",
      " |      Boosting learning rate (xgb's \"eta\")\n",
      " |  n_estimators : int\n",
      " |      Number of trees to fit.\n",
      " |  verbosity : int\n",
      " |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |  silent : boolean\n",
      " |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      " |  objective : string or callable\n",
      " |      Specify the learning task and the corresponding learning objective or\n",
      " |      a custom objective function to be used (see note below).\n",
      " |  booster: string\n",
      " |      Specify which booster to use: gbtree, gblinear or dart.\n",
      " |  nthread : int\n",
      " |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      " |  n_jobs : int\n",
      " |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      " |  gamma : float\n",
      " |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |  min_child_weight : int\n",
      " |      Minimum sum of instance weight(hessian) needed in a child.\n",
      " |  max_delta_step : int\n",
      " |      Maximum delta step we allow each tree's weight estimation to be.\n",
      " |  subsample : float\n",
      " |      Subsample ratio of the training instance.\n",
      " |  colsample_bytree : float\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |  colsample_bylevel : float\n",
      " |      Subsample ratio of columns for each level.\n",
      " |  colsample_bynode : float\n",
      " |      Subsample ratio of columns for each split.\n",
      " |  reg_alpha : float (xgb's alpha)\n",
      " |      L1 regularization term on weights\n",
      " |  reg_lambda : float (xgb's lambda)\n",
      " |      L2 regularization term on weights\n",
      " |  scale_pos_weight : float\n",
      " |      Balancing of positive and negative weights.\n",
      " |  base_score:\n",
      " |      The initial prediction score of all instances, global bias.\n",
      " |  seed : int\n",
      " |      Random number seed.  (Deprecated, please use random_state)\n",
      " |  random_state : int\n",
      " |      Random number seed.  (replaces seed)\n",
      " |  missing : float, optional\n",
      " |      Value in the data which needs to be present as a missing value. If\n",
      " |      None, defaults to np.nan.\n",
      " |  importance_type: string, default \"gain\"\n",
      " |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      " |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      " |  \\*\\*kwargs : dict, optional\n",
      " |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      " |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      " |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      " |      will result in a TypeError.\n",
      " |  \n",
      " |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      " |          passed via this argument will interact properly with scikit-learn.\n",
      " |  \n",
      " |  Note\n",
      " |  ----\n",
      " |  A custom objective function can be provided for the ``objective``\n",
      " |  parameter. In this case, it should have the signature\n",
      " |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |  y_true: array_like of shape [n_samples]\n",
      " |      The target values\n",
      " |  y_pred: array_like of shape [n_samples]\n",
      " |      The predicted values\n",
      " |  \n",
      " |  grad: array_like of shape [n_samples]\n",
      " |      The value of the gradient for each sample point.\n",
      " |  hess: array_like of shape [n_samples]\n",
      " |      The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the `fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      " |      When **eval_metric** is also passed to the `fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |          clf = xgb.XGBClassifier(**param_dist)\n",
      " |      \n",
      " |          clf.fit(X_train, y_train,\n",
      " |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |                  eval_metric='logloss',\n",
      " |                  verbose=True)\n",
      " |      \n",
      " |          evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable **evals_result** will contain\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      " |      Fit gradient boosting classifier\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          Weight for each instance\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) pairs to use as a validation set for\n",
      " |          early-stopping\n",
      " |      sample_weight_eval_set : list, optional\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      " |          instance weights on the i-th validation set.\n",
      " |      eval_metric : str, callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      " |          signature is func(y_predicted, y_true) where y_true will be a\n",
      " |          DMatrix object such that you may need to call the get_label\n",
      " |          method. It must return a str, value pair where the str is a name\n",
      " |          for the evaluation and value is the value of the evaluation\n",
      " |          function. This objective is always minimized.\n",
      " |      early_stopping_rounds : int, optional\n",
      " |          Activates early stopping. Validation error needs to decrease at\n",
      " |          least every <early_stopping_rounds> round(s) to continue training.\n",
      " |          Requires at least one item in evals. If there's more than one,\n",
      " |          will use the last. If early stopping occurs, the model will have\n",
      " |          three additional fields: bst.best_score, bst.best_iteration and\n",
      " |          bst.best_ntree_limit (bst.best_ntree_limit is the ntree_limit parameter\n",
      " |          default value in predict method if not any other value is specified).\n",
      " |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      " |          and/or num_class appears in the parameters)\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      " |          metric measured on the validation set to stderr.\n",
      " |      xgb_model : str\n",
      " |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      callbacks : list of callback functions\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      " |          Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      " |  \n",
      " |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      " |      Predict with `data`.\n",
      " |      \n",
      " |      .. note:: This function is not thread safe.\n",
      " |      \n",
      " |        For each booster object, predict can only be called from one thread.\n",
      " |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      " |        of model object and then call ``predict()``.\n",
      " |      \n",
      " |      .. note:: Using ``predict()`` with DART booster\n",
      " |      \n",
      " |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      " |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      " |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      " |        a nonzero value, e.g.\n",
      " |      \n",
      " |        .. code-block:: python\n",
      " |      \n",
      " |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DMatrix\n",
      " |          The dmatrix storing the input.\n",
      " |      output_margin : bool\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      " |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      " |          Otherwise, it is assumed that the feature_names are the same.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |  \n",
      " |  predict_proba(self, data, ntree_limit=None, validate_features=True)\n",
      " |      Predict the probability of each `data` example being of a given class.\n",
      " |      \n",
      " |      .. note:: This function is not thread safe\n",
      " |      \n",
      " |          For each booster object, predict can only be called from one thread.\n",
      " |          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      " |          of model object and then call predict\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DMatrix\n",
      " |          The dmatrix storing the input.\n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      " |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      " |          Otherwise, it is assumed that the feature_names are the same.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |          a numpy array with the probability of each data example being of a given class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  apply(self, X, ntree_limit=0)\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  get_booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self)\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep=False)\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost type parameters.\n",
      " |  \n",
      " |  load_model(self, fname)\n",
      " |      Load the model from a file.\n",
      " |      \n",
      " |      The model is loaded from an XGBoost internal binary format which is\n",
      " |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      " |      the Python Booster object (such as feature names) will not be loaded.\n",
      " |      Label encodings (text labels to numeric labels) will be also lost.\n",
      " |      **If you are using only the Python interface, we recommend pickling the\n",
      " |      model object for best results.**\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or a memory buffer\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  save_model(self, fname)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal binary format which is\n",
      " |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      " |      the Python Booster object (such as feature names) will not be loaded.\n",
      " |      Label encodings (text labels to numeric labels) will be also lost.\n",
      " |      **If you are using only the Python interface, we recommend pickling the\n",
      " |      model object for best results.**\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      " |      the full range of xgboost parameters that are not defined as member variables\n",
      " |      in sklearn grid search.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from XGBModel:\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property\n",
      " |      \n",
      " |      .. note:: Feature importance is defined only for tree boosters\n",
      " |      \n",
      " |          Feature importance is only defined when the decision tree model is chosen as base\n",
      " |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      " |          as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]``\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test_test = xgb_model_test.predict_proba(testseries_features_new[:,importance_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.002082581469681624\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(p_test_test[:,1:])>0.98).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predict2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_new = xgb.XGBClassifier(n_jobs=10, verbosity=2, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,) 2397122.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,) 4841141.0\n"
     ]
    }
   ],
   "source": [
    "ratio = round((len(timeseries_features_label_new) - sum(vital_label)) / sum(vital_label))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label_new)) - timeseries_features_label_new\n",
    "print(non_anomaly,non_anomaly.shape, sum(non_anomaly))\n",
    "extremely_sample_ratio = non_anomaly + vital_label * ratio\n",
    "print(extremely_sample_ratio,extremely_sample_ratio.shape, sum(extremely_sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002117664439357624\n"
     ]
    }
   ],
   "source": [
    "print(sum(vital_label)/len(vital_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:32:01] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:06] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:10] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:14] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:18] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:23] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:27] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:32] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:36] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:40] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:49] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:32:58] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:02] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:07] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:11] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:15] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:19] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:24] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:28] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:33] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:37] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:41] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:49] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:33:58] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:02] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:06] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:11] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:15] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:19] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:24] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:28] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:32] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:36] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:41] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:49] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:53] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:34:58] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:02] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:06] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:10] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:15] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:19] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:23] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:28] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:32] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:36] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:41] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:49] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:35:58] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:02] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:06] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:11] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:15] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:19] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:24] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:28] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:32] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:36:36] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:41] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:49] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:36:58] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:02] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:07] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:11] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:15] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:19] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:24] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:28] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:32] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:36] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:41] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:49] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:37:58] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:02] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:06] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:11] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:15] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:19] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:24] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:28] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:32] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:36] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:41] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:45] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:49] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:54] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:38:58] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:39:02] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[10:39:07] INFO: /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=10,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=2)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_new.fit(timeseries_features_new[:,importance_index], timeseries_features_label_new, sample_weight = extremely_sample_ratio, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = xgb_model_new.predict_proba(timeseries_features_new[:,importance_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2450341,) [0 0 0 ... 0 0 0]\n",
      "0.003058757944302446\n",
      "0.7934623082054704\n",
      "0.111745805069618\n",
      "0.19590209836281583\n"
     ]
    }
   ],
   "source": [
    "train_data_check = (np.ravel(y_new[:,1:])>0.93).astype(int)\n",
    "print(train_data_check.shape, train_data_check)\n",
    "print(sum(train_data_check)/len(train_data_check))\n",
    "print(precision_score(timeseries_features_label_new, train_data_check))\n",
    "print(recall_score(timeseries_features_label_new, train_data_check))\n",
    "print(f1_score(timeseries_features_label_new, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_new = xgb_model_new.predict_proba(testseries_features_new[:,importance_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.001924339772088838\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(p_new[:,1:])>0.98).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictnew.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 1.0320 - binary_accuracy: 0.7401\n",
      "Epoch 2/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.8045 - binary_accuracy: 0.8787\n",
      "Epoch 3/30\n",
      "2450341/2450341 [==============================] - 23s 9us/step - loss: 0.7462 - binary_accuracy: 0.9032\n",
      "Epoch 4/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.7042 - binary_accuracy: 0.9097\n",
      "Epoch 5/30\n",
      "2450341/2450341 [==============================] - 23s 10us/step - loss: 0.6787 - binary_accuracy: 0.9093\n",
      "Epoch 6/30\n",
      "2450341/2450341 [==============================] - 23s 10us/step - loss: 0.6591 - binary_accuracy: 0.9131\n",
      "Epoch 7/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.6362 - binary_accuracy: 0.9149\n",
      "Epoch 8/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.6148 - binary_accuracy: 0.9125\n",
      "Epoch 9/30\n",
      "2450341/2450341 [==============================] - 23s 10us/step - loss: 0.6037 - binary_accuracy: 0.9137\n",
      "Epoch 10/30\n",
      "2450341/2450341 [==============================] - 23s 10us/step - loss: 0.5924 - binary_accuracy: 0.9159\n",
      "Epoch 11/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.5777 - binary_accuracy: 0.9150\n",
      "Epoch 12/30\n",
      "2450341/2450341 [==============================] - 23s 9us/step - loss: 0.5657 - binary_accuracy: 0.9163\n",
      "Epoch 13/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.5544 - binary_accuracy: 0.9167\n",
      "Epoch 14/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.5524 - binary_accuracy: 0.9175\n",
      "Epoch 15/30\n",
      "2450341/2450341 [==============================] - 23s 10us/step - loss: 0.5400 - binary_accuracy: 0.9176\n",
      "Epoch 16/30\n",
      "2450341/2450341 [==============================] - 23s 10us/step - loss: 0.5330 - binary_accuracy: 0.9190\n",
      "Epoch 17/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.5223 - binary_accuracy: 0.9216\n",
      "Epoch 18/30\n",
      "2450341/2450341 [==============================] - 23s 9us/step - loss: 0.5190 - binary_accuracy: 0.9237\n",
      "Epoch 19/30\n",
      "2450341/2450341 [==============================] - 23s 10us/step - loss: 0.5087 - binary_accuracy: 0.9223\n",
      "Epoch 20/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.5081 - binary_accuracy: 0.9248\n",
      "Epoch 21/30\n",
      "2450341/2450341 [==============================] - 23s 9us/step - loss: 0.5056 - binary_accuracy: 0.9255\n",
      "Epoch 22/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.4914 - binary_accuracy: 0.9253\n",
      "Epoch 23/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.5013 - binary_accuracy: 0.9256\n",
      "Epoch 24/30\n",
      "2450341/2450341 [==============================] - 23s 9us/step - loss: 0.4921 - binary_accuracy: 0.9286\n",
      "Epoch 25/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.4842 - binary_accuracy: 0.9271\n",
      "Epoch 26/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.4784 - binary_accuracy: 0.9294\n",
      "Epoch 27/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.4836 - binary_accuracy: 0.9304\n",
      "Epoch 28/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.4675 - binary_accuracy: 0.9305\n",
      "Epoch 29/30\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 0.4676 - binary_accuracy: 0.9315\n",
      "Epoch 30/30\n",
      "2450341/2450341 [==============================] - 23s 9us/step - loss: 0.4693 - binary_accuracy: 0.9302\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim = 128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "h = model.fit(timeseries_features_new, timeseries_features_label_new, epochs=30, batch_size=5000, verbose=1,\n",
    "               sample_weight=extremely_sample_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450341/2450341 [==============================] - 10s 4us/step\n"
     ]
    }
   ],
   "source": [
    "p_model = model.predict(timeseries_features_new, batch_size=5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007050039157815178\n",
      "0.4663386396526773\n",
      "0.15137450910389147\n",
      "0.2285584588759327\n"
     ]
    }
   ],
   "source": [
    "train_data_check = np.ravel(p_model>0.85).astype(int)\n",
    "print(sum(train_data_check)/len(train_data_check))\n",
    "print(precision_score(timeseries_features_label_new, train_data_check))\n",
    "print(recall_score(timeseries_features_label_new, train_data_check))\n",
    "print(f1_score(timeseries_features_label_new, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319237/2319237 [==============================] - 7s 3us/step\n"
     ]
    }
   ],
   "source": [
    "p_model_test = m.predict(testseries_features_new,batch_size=5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.0037978007422268616\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(p_model_test)>0.99).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
