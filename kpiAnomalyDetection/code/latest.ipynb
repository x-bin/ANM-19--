{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np   \n",
    "import sklearn\n",
    "import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../input/train.csv')\n",
    "test_data = pd.read_csv('../../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list, id_indexes = np.unique(train_data['KPI ID'], return_index=True)\n",
    "id_indexes.sort()\n",
    "id_indexes = np.append(id_indexes, len(train_data))   \n",
    "timeseries_all = []\n",
    "timeseries_label = []\n",
    "\n",
    "for i in np.arange(len(id_indexes)-1):\n",
    "    timeseries_all.append(np.asarray(train_data['value'][id_indexes[i]:id_indexes[i+1]]))\n",
    "    timeseries_label.append(np.asarray(train_data['label'][id_indexes[i]:id_indexes[i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id_list, test_id_indexes = np.unique(test_data['KPI ID'], return_index=True)\n",
    "test_id_indexes.sort()\n",
    "test_id_indexes = np.append(test_id_indexes, len(test_data))   \n",
    "testseries_all = []\n",
    "\n",
    "for i in np.arange(len(test_id_indexes)-1):\n",
    "    testseries_all.append(np.asarray(test_data['value'][test_id_indexes[i]:test_id_indexes[i+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "def get_feature_AddES_residuals(time_series):\n",
    "    predict = ExponentialSmoothing(time_series, trend='add').fit(smoothing_level=1)\n",
    "    return time_series - predict.fittedvalues\n",
    "\n",
    "def get_feature_SimpleES_residuals(time_series):\n",
    "    predict = SimpleExpSmoothing(time_series).fit(smoothing_level=1)\n",
    "    return time_series - predict.fittedvalues\n",
    "\n",
    "def get_feature_Holt_residuals(time_series):\n",
    "    predict = Holt(time_series).fit(smoothing_level=1)\n",
    "    return time_series - predict.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_get_timeseries_features(time_series, time_series_label, Windows, delay):\n",
    "  \n",
    "    data = []\n",
    "    data_label = []\n",
    "    data_label_vital = []\n",
    "    \n",
    "    start_point = 2*max(Windows) - 1\n",
    "    start_accum = sum(time_series[0:start_point])\n",
    "    \n",
    "    time_series_AddES_residuals = get_feature_AddES_residuals(time_series)\n",
    "    time_series_SimpleES_residuals = get_feature_SimpleES_residuals(time_series)\n",
    "    time_Series_Holt_residuals = get_feature_Holt_residuals(time_series)\n",
    "    \n",
    "    for i in np.arange(start_point, len(time_series)):        \n",
    "        datum = []\n",
    "        datum_label = time_series_label[i]        \n",
    "        \n",
    "        diff_plain = time_series[i] - time_series[i-1]\n",
    "        start_accum = start_accum + time_series[i]\n",
    "        mean_accum = (start_accum)/(i+1)\n",
    "        \n",
    "        datum.append(time_series_AddES_residuals[i])\n",
    "        datum.append(time_series_SimpleES_residuals[i])\n",
    "        datum.append(time_Series_Holt_residuals[i])\n",
    "\n",
    "        datum.append(time_series[i])\n",
    "        \n",
    "\n",
    "        datum.append(diff_plain)\n",
    "        \n",
    "        datum.append(diff_plain/(time_series[i-1] + 1e-8))  \n",
    "\n",
    "        datum.append(diff_plain - (time_series[i-1] - time_series[i-2]))\n",
    " \n",
    "        datum.append(time_series[i] - mean_accum)\n",
    "\n",
    "\n",
    "        for k in Windows:\n",
    "            mean_w = np.mean(time_series[i-k+1:i+1])\n",
    "            var_w = np.mean((np.asarray(time_series[i-k+1:i+1]) - mean_w)**2)\n",
    " \n",
    "            \n",
    "            mean_w_and_1 = mean_w + (time_series[i-k]-time_series[i])/k\n",
    "            var_w_and_1 = np.mean((np.asarray(time_series[i-k:i]) - mean_w_and_1)**2)\n",
    "\n",
    "            \n",
    "            mean_2w = np.mean(time_series[i-2*k+1:i-k+1])\n",
    "            var_2w = np.mean((np.asarray(time_series[i-2*k+1:i-k+1]) - mean_2w)**2)\n",
    "            \n",
    "            \n",
    "            diff_mean_1 = mean_w - mean_w_and_1\n",
    "            diff_var_1 = var_w - var_w_and_1\n",
    "            \n",
    "            diff_mean_w = mean_w - mean_2w\n",
    "            diff_var_w = var_w - var_2w\n",
    "            \n",
    "            datum.append(mean_w)  \n",
    "            \n",
    "            datum.append(var_w)\n",
    "            \n",
    "            datum.append(diff_mean_1)\n",
    "            \n",
    "            datum.append(diff_mean_1/(mean_w_and_1 + 1e-8))\n",
    "            \n",
    "            datum.append(diff_var_1)\n",
    "            \n",
    "            datum.append(diff_var_1/(var_w_and_1 + 1e-8))\n",
    "            \n",
    "            datum.append(diff_mean_w)\n",
    "            \n",
    "            datum.append(diff_mean_w/(mean_2w + 1e-8))\n",
    "            \n",
    "            datum.append(diff_var_w)\n",
    "            \n",
    "            datum.append(diff_var_w/(var_2w + 1e-8))\n",
    "            \n",
    "            \n",
    "            datum.append(time_series[i] - mean_w_and_1)\n",
    "            \n",
    "            datum.append(time_series[i] - mean_2w)\n",
    "\n",
    "        data.append(np.asarray(datum))\n",
    "        data_label.append(np.asarray(datum_label))\n",
    "\n",
    "        if datum_label == 1 and sum(time_series_label[i-delay:i]) < delay + 1:\n",
    "            data_label_vital.append(np.asarray(1))\n",
    "        else:\n",
    "            data_label_vital.append(np.asarray(0))\n",
    "            \n",
    "    return data, data_label, data_label_vital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.asarray([2, 5, 10, 25, 50, 100, 200, 300, 400, 500])\n",
    "delay = 7\n",
    "scaler_list_new = [] \n",
    "timeseries_features_new = []\n",
    "timeseries_features_label_new = []\n",
    "timeseries_features_label_vital_new = []\n",
    "\n",
    "for i in range(len(timeseries_all)):\n",
    "    print(i,len(timeseries_all[i]),len(scaler_list_new),len(timeseries_features_new),len(timeseries_features_label_new),\n",
    "                len(timeseries_features_label_vital_new))\n",
    "    features_temp,label_temp,label_vital_temp = new_get_timeseries_features(timeseries_all[i], timeseries_label[i], W, delay) \n",
    "    assert(len(features_temp)==len(label_temp))\n",
    "    assert(len(label_temp) == len(label_vital_temp))\n",
    "    scaler_temp = StandardScaler()\n",
    "    features_temp = scaler_temp.fit_transform(features_temp)\n",
    "    scaler_list_new.append(scaler_temp)\n",
    "    if i==0:\n",
    "        timeseries_features_new = features_temp\n",
    "    else:\n",
    "        timeseries_features_new = np.concatenate((timeseries_features_new, features_temp), axis = 0)\n",
    "        \n",
    "    timeseries_features_label_new = timeseries_features_label_new + label_temp\n",
    "    timeseries_features_label_vital_new = timeseries_features_label_vital_new + label_vital_temp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_get_test_features(time_series, Windows):\n",
    "  \n",
    "    data = []\n",
    "    \n",
    "    start_point = 2*max(Windows) - 1\n",
    "    start_accum = sum(time_series[0:start_point])\n",
    "    \n",
    "    # features from tsa models\n",
    "    #time_series_SARIMA_residuals = get_feature_SARIMA_residuals(time_series)\n",
    "    time_series_AddES_residuals = get_feature_AddES_residuals(time_series)\n",
    "    time_series_SimpleES_residuals = get_feature_SimpleES_residuals(time_series)\n",
    "    time_Series_Holt_residuals = get_feature_Holt_residuals(time_series)\n",
    "    \n",
    "    for i in np.arange(start_point, len(time_series)):        \n",
    "        # the datum to put into the data pool\n",
    "        datum = []        \n",
    "        \n",
    "        # fill the datum with f01-f09\n",
    "        diff_plain = time_series[i] - time_series[i-1]\n",
    "        start_accum = start_accum + time_series[i]\n",
    "        mean_accum = (start_accum)/(i+1)\n",
    "        \n",
    "        # f01-f04: residuals\n",
    "        #datum.append(time_series_SARIMA_residuals[i])\n",
    "        datum.append(time_series_AddES_residuals[i])\n",
    "        datum.append(time_series_SimpleES_residuals[i])\n",
    "        datum.append(time_Series_Holt_residuals[i])\n",
    "        # f05: logarithm\n",
    "        datum.append(time_series[i])\n",
    "        \n",
    "        # f06: diff\n",
    "        datum.append(diff_plain)\n",
    "        # f07: diff percentage\n",
    "        datum.append(diff_plain/(time_series[i-1] + 1e-8))  # to avoid 0, plus 1e-10\n",
    "        # f08: diff of diff - derivative\n",
    "        datum.append(diff_plain - (time_series[i-1] - time_series[i-2]))\n",
    "        # f09: diff of accumulated mean and current value\n",
    "        datum.append(time_series[i] - mean_accum)\n",
    "\n",
    "        # fill the datum with features related to windows\n",
    "        # loop over different windows size to fill the datum\n",
    "        for k in Windows:\n",
    "            mean_w = np.mean(time_series[i-k+1:i+1])\n",
    "            var_w = np.mean((np.asarray(time_series[i-k+1:i+1]) - mean_w)**2)\n",
    "            #var_w = np.var(time_series[i-k:i+1])\n",
    "            \n",
    "            mean_w_and_1 = mean_w + (time_series[i-k]-time_series[i])/k\n",
    "            var_w_and_1 = np.mean((np.asarray(time_series[i-k:i]) - mean_w_and_1)**2)\n",
    "            #mean_w_and_1 = np.mean(time_series[i-k-1:i])\n",
    "            #var_w_and_1 = np.var(time_series[i-k-1:i])\n",
    "            \n",
    "            mean_2w = np.mean(time_series[i-2*k+1:i-k+1])\n",
    "            var_2w = np.mean((np.asarray(time_series[i-2*k+1:i-k+1]) - mean_2w)**2)\n",
    "            #var_2w = np.var(time_series[i-2*k:i-k+1])\n",
    "            \n",
    "            # diff of sliding windows\n",
    "            diff_mean_1 = mean_w - mean_w_and_1\n",
    "            diff_var_1 = var_w - var_w_and_1\n",
    "            \n",
    "            # diff of jumping windows\n",
    "            diff_mean_w = mean_w - mean_2w\n",
    "            diff_var_w = var_w - var_2w\n",
    "            \n",
    "            # f1\n",
    "            datum.append(mean_w)  # [0:2] is [0,1]\n",
    "            # f2\n",
    "            datum.append(var_w)\n",
    "            # f3\n",
    "            datum.append(diff_mean_1)\n",
    "            # f4\n",
    "            datum.append(diff_mean_1/(mean_w_and_1 + 1e-8))\n",
    "            # f5\n",
    "            datum.append(diff_var_1)\n",
    "            # f6\n",
    "            datum.append(diff_var_1/(var_w_and_1 + 1e-8))\n",
    "            # f7\n",
    "            datum.append(diff_mean_w)\n",
    "            # f8\n",
    "            datum.append(diff_mean_w/(mean_2w + 1e-8))\n",
    "            # f9\n",
    "            datum.append(diff_var_w)\n",
    "            # f10\n",
    "            datum.append(diff_var_w/(var_2w + 1e-8))\n",
    "            \n",
    "            # diff of sliding/jumping windows and current value\n",
    "            # f11\n",
    "            datum.append(time_series[i] - mean_w_and_1)\n",
    "            # f12\n",
    "            datum.append(time_series[i] - mean_2w)\n",
    "\n",
    "        data.append(np.asarray(datum))\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testseries_features_new = []\n",
    "for i in range(len(testseries_all)):\n",
    "    print(i, len(testseries_all[i]), len(testseries_features_new))\n",
    "    features_temp = new_get_test_features(testseries_all[i], W)\n",
    "    features_temp = scaler_list_new[i].transform(features_temp)\n",
    "    if i==0:\n",
    "        testseries_features_new = features_temp\n",
    "    else:\n",
    "        testseries_features_new = np.concatenate((testseries_features_new, features_temp), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_features_label_new = np.array(timeseries_features_label_new)\n",
    "timeseries_features_label_vital_new = np.array(timeseries_features_label_vital_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features_diff = len(test_data) - len(testseries_features_new)\n",
    "print(data_features_diff)\n",
    "data_features_diff_avg = int(data_features_diff / len(testseries_all))\n",
    "print(data_features_diff_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_non = len(timeseries_features_label_new) - sum(timeseries_features_label_new)\n",
    "vital_ratio = round((sum_non - sum(timeseries_features_label_new) + sum(timeseries_features_label_vital_new))/sum(timeseries_features_label_vital_new))\n",
    "sample_ratio_new = vital_ratio * vital_label + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(128, input_dim = 128))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dropout(0.5))\n",
    "\n",
    "m.add(Dense(64))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dropout(0.5))\n",
    "\n",
    "m.add(Dense(1))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "h = m.fit(timeseries_features_new, timeseries_features_label_new, epochs=30, batch_size=5000, verbose=1,\n",
    "               sample_weight=sample_ratio_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = m.predict(timeseries_features_new, batch_size=5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_check = np.ravel(p>0.96).astype(int)\n",
    "print(sum(train_data_check)/len(train_data_check))\n",
    "print(precision_score(timeseries_features_label_new, train_data_check))\n",
    "print(recall_score(timeseries_features_label_new, train_data_check))\n",
    "print(f1_score(timeseries_features_label_new, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index = 0\n",
    "evaluation_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(timeseries_all)):\n",
    "    next_index += len(timeseries_all[i]) - data_features_diff_avg\n",
    "    evaluation_new = np.concatenate((evaluation_new, train_data_check[last_index : next_index]))\n",
    "    print(len(evaluation_new),next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(timeseries_all)-1:\n",
    "        evaluation_new = np.concatenate((evaluation_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(evaluation_new))\n",
    "assert(len(evaluation_new) == len(train_data))\n",
    "evaluation_new = evaluation_new.astype(int)\n",
    "evaluation_df = pd.DataFrame({'KPI ID': train_data['KPI ID'], \n",
    "                         'timestamp': train_data['timestamp'], \n",
    "                         'predict': evaluation_new})\n",
    "evaluation_df.to_csv('evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluation.py \"../../input/train.csv\" \"evaluation.csv\" 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_t = m.predict(testseries_features_new,batch_size=5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_flagm = (np.ravel(pm_t)>0.96).astype(int)\n",
    "print(predict_flagm)\n",
    "print(sum(predict_flagm)/len(predict_flagm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flagm[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictDNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_jobs=10, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = round((len(timeseries_features_label_new) - sum(timeseries_features_label_new)) * 0.05 / sum(timeseries_features_label_new))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label_new)) - timeseries_features_label_new\n",
    "print(non_anomaly,non_anomaly.shape)\n",
    "xgb_sample_ratio = (239*ratio) * vital_label + non_anomaly\n",
    "print(xgb_sample_ratio,sum(xgb_sample_ratio))\n",
    "xgb_sample_ratio = xgb_sample_ratio + ratio * timeseries_features_label_new\n",
    "print(xgb_sample_ratio,sum(xgb_sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(timeseries_features_new, timeseries_features_label_new, sample_weight = xgb_sample_ratio, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = xgb_model.feature_importances_\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = 0\n",
    "sort_index = np.argsort(importance)\n",
    "importance_index = []\n",
    "for i in range(len(sort_index)):\n",
    "    if importance[sort_index[len(importance)-1-i]] > 0.001:\n",
    "        importance_index.append(sort_index[len(importance)-1-i])\n",
    "        sum1 += importance[sort_index[len(importance)-1-i]]\n",
    "        print(sort_index[len(importance)-1-i],importance[sort_index[len(importance)-1-i]],sum1)\n",
    "importance_index.sort()\n",
    "print(importance_index, len(importance_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t = xgb_model.predict_proba(testseries_features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_test = xgb.XGBClassifier(n_jobs=10, verbosity=2, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_test.fit(timeseries_features_new[:,importance_index], timeseries_features_label_new, sample_weight = sample_ratio_new, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test_test = xgb_model_test.predict_proba(testseries_features_new[:,importance_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_t1 = (np.ravel(p_t[:,1:])>0.98)\n",
    "predict_t2 = np.ravel(pm_t)>0.96\n",
    "predict_t3 = (np.ravel(p_test_test[:,1:]) > 0.9325)\n",
    "predict_xg = ((predict_t1 | predict_t3)).astype(int)\n",
    "predict_flag = (predict_t2 | predict_xg).astype(int)\n",
    "print(sum(predict_t2)/len(predict_t2), sum(predict_xg)/len(predict_xg))\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predict.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
